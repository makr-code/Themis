# Streaming JSONL LLM Training Endpoint

## Overview

ThemisDB JSONL Export API unterstÃ¼tzt **echtes Streaming** fÃ¼r on-demand LLM Training. Der LoRA/QLoRA Trainingsprozess kann Daten direkt aus der DB beziehen, ohne vollstÃ¤ndigen Export.

## Streaming-Architektur

### Bereits implementiert (Commit 6b4129b)
âœ… **Chunked Transfer Encoding** - Server streamt JSONL Zeile fÃ¼r Zeile  
âœ… **Keine Zwischenspeicherung** - Daten werden on-the-fly aus RocksDB gelesen  
âœ… **Backpressure Support** - Client-seitige Rate-Limitierung mÃ¶glich

### Neu: On-Demand Training mit Cache

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      HTTP Stream      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LoRA/QLoRA â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  ThemisDB    â”‚
â”‚  Training   â”‚   Chunked Transfer    â”‚  HTTP Server â”‚
â”‚  Process    â”‚                       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
      â”‚                                      â”‚
      â–¼                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Local Cache â”‚                       â”‚   RocksDB    â”‚
â”‚  (Optional) â”‚                       â”‚   Storage    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Implementierung

### HTTP Streaming Endpoint (bereits vorhanden)

```http
POST /api/export/jsonl_llm/stream
Authorization: Bearer <token>
Transfer-Encoding: chunked

{
  "theme": "Rechtssprechung",
  "from_date": "2020-01-01",
  "batch_size": 100,
  "stream_mode": "continuous"
}

Response 200 OK:
Transfer-Encoding: chunked
Content-Type: application/x-ndjson

{"instruction": "...", "output": "...", "weight": 1.2}\n
{"instruction": "...", "output": "...", "weight": 0.9}\n
...
```

### PyTorch DataLoader Integration

```python
import requests
from torch.utils.data import IterableDataset, DataLoader

class ThemisDBStreamDataset(IterableDataset):
    """
    On-Demand JSONL Streaming Dataset fÃ¼r LoRA/QLoRA Training
    
    Features:
    - Direkte DB-Verbindung (kein lokaler Export)
    - Optional: Lokaler Cache fÃ¼r wiederholte Epochen
    - Backpressure: Training-Geschwindigkeit bestimmt Query-Rate
    """
    
    def __init__(self, base_url, token, query_params, cache_dir=None):
        self.base_url = base_url
        self.token = token
        self.query_params = query_params
        self.cache_dir = cache_dir
        self.cache = []
        self.cache_enabled = cache_dir is not None
    
    def __iter__(self):
        # Wenn Cache existiert, nutze Cache
        if self.cache_enabled and len(self.cache) > 0:
            for item in self.cache:
                yield item
            return
        
        # Sonst: Stream von ThemisDB
        response = requests.post(
            f'{self.base_url}/api/export/jsonl_llm/stream',
            headers={'Authorization': f'Bearer {self.token}'},
            json=self.query_params,
            stream=True  # Wichtig: Streaming aktivieren
        )
        
        for line in response.iter_lines():
            if line:
                item = json.loads(line)
                
                # Optional: Cache fÃ¼r spÃ¤tere Epochen
                if self.cache_enabled:
                    self.cache.append(item)
                
                yield item

# Verwendung im Training
dataset = ThemisDBStreamDataset(
    base_url='https://themisdb',
    token=ADMIN_TOKEN,
    query_params={
        'theme': 'Rechtssprechung',
        'from_date': '2020-01-01',
        'batch_size': 100,
        'weighting': {'auto_weight_by_freshness': True}
    },
    cache_dir='./cache/epoch1'  # Optional: Cache fÃ¼r Epoch 2+
)

dataloader = DataLoader(dataset, batch_size=32)

# Training Loop
for epoch in range(num_epochs):
    for batch in dataloader:
        # Training step
        # Daten werden on-demand von ThemisDB gestreamt
        # Kein vollstÃ¤ndiger Export notwendig
        loss = train_step(batch)
```

### HuggingFace Datasets Integration

```python
from datasets import IterableDataset
import requests
import json

def themisdb_generator(base_url, token, query_params):
    """Generator fÃ¼r HuggingFace IterableDataset"""
    response = requests.post(
        f'{base_url}/api/export/jsonl_llm/stream',
        headers={'Authorization': f'Bearer {token}'},
        json=query_params,
        stream=True
    )
    
    for line in response.iter_lines():
        if line:
            yield json.loads(line)

# HuggingFace Dataset erstellen
dataset = IterableDataset.from_generator(
    themisdb_generator,
    gen_kwargs={
        'base_url': 'https://themisdb',
        'token': ADMIN_TOKEN,
        'query_params': {
            'theme': 'Immissionsschutz',
            'format': 'instruction_tuning'
        }
    }
)

# PEFT LoRA Training
from transformers import Trainer, TrainingArguments
from peft import LoraConfig, get_peft_model

trainer = Trainer(
    model=peft_model,
    args=training_args,
    train_dataset=dataset  # Streaming dataset!
)
trainer.train()
```

## Cache-Strategien

### 1. Kein Cache (Pure Streaming)
```python
# FÃ¼r sehr groÃŸe Datasets, die nicht in RAM passen
# Jede Epoche streamt neu von DB
dataset = ThemisDBStreamDataset(..., cache_dir=None)
```

### 2. RAM-Cache (erste Epoche)
```python
# Erste Epoche: Stream von DB + Cache in RAM
# Folgende Epochen: Aus RAM-Cache
dataset = ThemisDBStreamDataset(..., cache_dir=None)
dataset.cache_enabled = True  # Aktiviert RAM-Cache
```

### 3. Disk-Cache (Persistenz)
```python
# Erste Epoche: Stream von DB + Cache auf Disk
# SpÃ¤tere Trainings: Aus Disk-Cache
dataset = ThemisDBStreamDataset(..., cache_dir='./cache/rechtssprechung')
```

### 4. Hybrid (LRU-Cache)
```python
from functools import lru_cache

class CachedThemisDBDataset(ThemisDBStreamDataset):
    @lru_cache(maxsize=10000)
    def _fetch_item(self, index):
        # Automatischer LRU-Cache fÃ¼r hÃ¤ufig genutzte Items
        pass
```

## Performance-Optimierungen

### Server-Seite (ThemisDB)

```cpp
// export_api_handler.cpp - Bereits implementiert
void handleStreamExport(const HttpRequest& req, HttpResponse& res) {
    // Chunked Transfer Encoding
    res.setHeader("Transfer-Encoding", "chunked");
    res.setHeader("Content-Type", "application/x-ndjson");
    
    // Stream Query-Ergebnisse
    size_t batch_size = config.batch_size;
    std::vector<BaseEntity> batch;
    
    while (query.hasMore()) {
        batch = query.fetchBatch(batch_size);  // Batched DB access
        
        for (const auto& entity : batch) {
            std::string jsonl_line = exporter.formatEntity(entity);
            res.writeChunk(jsonl_line + "\n");  // Sofortiges Senden
        }
    }
    
    res.endChunks();
}
```

### Client-Seite (Python)

```python
class OptimizedStreamDataset(IterableDataset):
    def __init__(self, ..., prefetch_size=1000):
        self.prefetch_size = prefetch_size
        self.buffer = []
    
    def __iter__(self):
        response = requests.post(..., stream=True)
        
        # Prefetching fÃ¼r bessere Performance
        for line in response.iter_lines(chunk_size=self.prefetch_size):
            if line:
                self.buffer.append(json.loads(line))
                
                if len(self.buffer) >= self.prefetch_size:
                    # Batch yielden
                    for item in self.buffer:
                        yield item
                    self.buffer = []
```

## Vorteile

### 1. Speicher-Effizienz
- âŒ Kein vollstÃ¤ndiger Export (100 GB JSONL vermieden)
- âœ… Nur aktuelle Batch im RAM (~10-100 MB)
- âœ… Training beginnt sofort (kein Warten auf Export)

### 2. AktualitÃ¤t
- âœ… Immer neueste Daten aus DB
- âœ… Inkrementelle Updates wÃ¤hrend Training
- âœ… Dynamische Filterung (z.B. nur Daten nach Datum X)

### 3. FlexibilitÃ¤t
- âœ… Verschiedene Queries pro Epoche
- âœ… A/B Testing mit verschiedenen Gewichtungen
- âœ… Adaptive Sampling basierend auf Trainings-Metriken

## Beispiel: Multi-Thema Training

```python
# Training mit mehreren Themen im Wechsel
themes = ['Rechtssprechung', 'Immissionsschutz', 'Datenschutz']

for epoch in range(num_epochs):
    for theme in themes:
        dataset = ThemisDBStreamDataset(
            query_params={
                'theme': theme,
                'from_date': get_recent_date(days=365),
                'weighting': {'auto_weight_by_freshness': True}
            },
            cache_dir=f'./cache/{theme}_epoch{epoch}'
        )
        
        # Training auf theme-spezifischem Dataset
        trainer.train(dataset, max_steps=1000)
```

## Implementierungsstatus

âœ… **Server-seitiges Streaming** - Bereits implementiert (Commit 6b4129b)  
âœ… **Chunked Transfer Encoding** - Funktioniert  
âœ… **Backpressure Support** - Client bestimmt Rate  
ğŸ“ **Python Client Library** - Beispielcode oben  
ğŸ“ **Cache-Strategien** - Implementierbar durch Client

## NÃ¤chste Schritte

1. **Python Package:** `themisdb-datasets` (PyPI)
   - ThemisDBStreamDataset
   - HuggingFace Integration
   - Auto-Caching

2. **WebSocket Support** (Optional)
   - Bi-direktionale Kommunikation
   - Training-Feedback an DB (welche Samples effektiv)

3. **Adaptive Sampling** (Optional)
   - Server tracked welche Samples schwer zu lernen sind
   - Dynamische Gewichtung wÃ¤hrend Training
